{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 96\n",
    "embedding_length = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.float32,[None,sequence_length,embedding_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Build RNN Cell](https://blog.csdn.net/abclhq2005/article/details/78683530)\n",
    "`tf.contrib.rnn.BasicLSTMCell`與`tf.contrib.rnn.LSTMCell`不同在於新增了3個東西\n",
    "1. `use_peepholes=False`：窺視孔，於2001年`Gers`提出，相較於原本有3個gate的LSTM多出了一個gate，預設為`False`，也就是普通的LSTM\n",
    "2. `cell_clip=None`：`c = clip_ops.clip_by_value(c, -self._cell_clip, self._cell_clip)`，將`cell`的值限制在正負`self._cell_clip`內，可以視為一種防止梯度爆炸的方法。\n",
    "3. `num_proj=None,proj_clip=None`：在cell輸出`hidden_state`後再增加一層線性變換，並限制輸出的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BasicLSTMCell in module tensorflow.python.ops.rnn_cell_impl:\n",
      "\n",
      "class BasicLSTMCell(LayerRNNCell)\n",
      " |  Basic LSTM recurrent network cell.\n",
      " |  \n",
      " |  The implementation is based on: http://arxiv.org/abs/1409.2329.\n",
      " |  \n",
      " |  We add forget_bias (default: 1) to the biases of the forget gate in order to\n",
      " |  reduce the scale of forgetting in the beginning of the training.\n",
      " |  \n",
      " |  It does not allow cell clipping, a projection layer, and does not\n",
      " |  use peep-hole connections: it is the basic baseline.\n",
      " |  \n",
      " |  For advanced models, please use the full @{tf.nn.rnn_cell.LSTMCell}\n",
      " |  that follows.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BasicLSTMCell\n",
      " |      LayerRNNCell\n",
      " |      RNNCell\n",
      " |      tensorflow.python.layers.base.Layer\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.training.checkpointable.base.CheckpointableBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, num_units, forget_bias=1.0, state_is_tuple=True, activation=None, reuse=None, name=None, dtype=None)\n",
      " |      Initialize the basic LSTM cell.\n",
      " |      \n",
      " |      Args:\n",
      " |        num_units: int, The number of units in the LSTM cell.\n",
      " |        forget_bias: float, The bias added to forget gates (see above).\n",
      " |          Must set to `0.0` manually when restoring from CudnnLSTM-trained\n",
      " |          checkpoints.\n",
      " |        state_is_tuple: If True, accepted and returned states are 2-tuples of\n",
      " |          the `c_state` and `m_state`.  If False, they are concatenated\n",
      " |          along the column axis.  The latter behavior will soon be deprecated.\n",
      " |        activation: Activation function of the inner states.  Default: `tanh`.\n",
      " |        reuse: (optional) Python boolean describing whether to reuse variables\n",
      " |          in an existing scope.  If not `True`, and the existing scope already has\n",
      " |          the given variables, an error is raised.\n",
      " |        name: String, the name of the layer. Layers with the same name will\n",
      " |          share weights, but to avoid mistakes we require reuse=True in such\n",
      " |          cases.\n",
      " |        dtype: Default dtype of the layer (default of `None` means use the type\n",
      " |          of the first input). Required when `build` is called before `call`.\n",
      " |      \n",
      " |        When restoring from CudnnLSTM-trained checkpoints, must use\n",
      " |        `CudnnCompatibleLSTMCell` instead.\n",
      " |  \n",
      " |  build(self, inputs_shape)\n",
      " |      Creates the variables of the layer.\n",
      " |  \n",
      " |  call(self, inputs, state)\n",
      " |      Long short-term memory cell (LSTM).\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: `2-D` tensor with shape `[batch_size, input_size]`.\n",
      " |        state: An `LSTMStateTuple` of state tensors, each shaped\n",
      " |          `[batch_size, num_units]`, if `state_is_tuple` has been set to\n",
      " |          `True`.  Otherwise, a `Tensor` shaped\n",
      " |          `[batch_size, 2 * num_units]`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A pair containing the new hidden state, and the new state (either a\n",
      " |          `LSTMStateTuple` or a concatenated state, depending on\n",
      " |          `state_is_tuple`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  output_size\n",
      " |      Integer or TensorShape: size of outputs produced by this cell.\n",
      " |  \n",
      " |  state_size\n",
      " |      size(s) of state(s) used by this cell.\n",
      " |      \n",
      " |      It can be represented by an Integer, a TensorShape or a tuple of Integers\n",
      " |      or TensorShapes.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LayerRNNCell:\n",
      " |  \n",
      " |  __call__(self, inputs, state, scope=None, *args, **kwargs)\n",
      " |      Run this RNN cell on inputs, starting from the given state.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: `2-D` tensor with shape `[batch_size, input_size]`.\n",
      " |        state: if `self.state_size` is an integer, this should be a `2-D Tensor`\n",
      " |          with shape `[batch_size, self.state_size]`.  Otherwise, if\n",
      " |          `self.state_size` is a tuple of integers, this should be a tuple\n",
      " |          with shapes `[batch_size, s] for s in self.state_size`.\n",
      " |        scope: optional cell scope.\n",
      " |        *args: Additional positional arguments.\n",
      " |        **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A pair containing:\n",
      " |      \n",
      " |        - Output: A `2-D` tensor with shape `[batch_size, self.output_size]`.\n",
      " |        - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n",
      " |          the arity and shapes of `state`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RNNCell:\n",
      " |  \n",
      " |  zero_state(self, batch_size, dtype)\n",
      " |      Return zero-filled state tensor(s).\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: int, float, or unit Tensor representing the batch size.\n",
      " |        dtype: the data type to use for the state.\n",
      " |      \n",
      " |      Returns:\n",
      " |        If `state_size` is an int or TensorShape, then the return value is a\n",
      " |        `N-D` tensor of shape `[batch_size, state_size]` filled with zeros.\n",
      " |      \n",
      " |        If `state_size` is a nested list or tuple, then the return value is\n",
      " |        a nested list or tuple (of the same structure) of `2-D` tensors with\n",
      " |        the shapes `[batch_size, s]` for each s in `state_size`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.layers.base.Layer:\n",
      " |  \n",
      " |  __deepcopy__(self, memo)\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      Note that `add_loss` is not supported when executing eagerly. Instead,\n",
      " |      variable regularizers may be added through `add_variable`. Activity\n",
      " |      regularization is not supported directly (but such losses may be returned\n",
      " |      from `Layer.call()`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors.\n",
      " |        inputs: If anything other than None is passed, it signals the losses\n",
      " |          are conditional on some of the layer's inputs,\n",
      " |          and thus they should only be run where these inputs are available.\n",
      " |          This is the case for activity regularization losses, for instance.\n",
      " |          If `None` is passed, the losses are assumed\n",
      " |          to be unconditional, and will apply across all dataflows of the layer\n",
      " |          (e.g. weight regularization losses).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  add_weight(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None, use_resource=None, partitioner=None)\n",
      " |      Adds a new variable to the layer, or gets an existing one; returns it.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: variable name.\n",
      " |        shape: variable shape.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: initializer instance (callable).\n",
      " |        regularizer: regularizer instance (callable).\n",
      " |        trainable: whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean, stddev).\n",
      " |          Note, if the current variable scope is marked as non-trainable\n",
      " |          then this parameter is ignored and any added variables are also\n",
      " |          marked as non-trainable.\n",
      " |        constraint: constraint instance (callable).\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        partitioner: (optional) partitioner instance (callable).  If\n",
      " |          provided, when the requested variable is created it will be split\n",
      " |          into multiple partitions according to `partitioner`.  In this case,\n",
      " |          an instance of `PartitionedVariable` is returned.  Available\n",
      " |          partitioners include `tf.fixed_size_partitioner` and\n",
      " |          `tf.variable_axis_size_partitioner`.  For more details, see the\n",
      " |          documentation of `tf.get_variable` and the  \"Variable Partitioners\n",
      " |          and Sharding\" section of the API guide.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable.  Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance.  If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.layers.base.Layer:\n",
      " |  \n",
      " |  graph\n",
      " |  \n",
      " |  scope_name\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops.\n",
      " |        inputs: If anything other than None is passed, it signals the updates\n",
      " |          are conditional on some of the layer's inputs,\n",
      " |          and thus they should only be run where these inputs are available.\n",
      " |          This is the case for BatchNormalization updates, for instance.\n",
      " |          If None, the updates will be taken into account unconditionally,\n",
      " |          and you are responsible for making sure that any dependency they might\n",
      " |          have is available at runtime.\n",
      " |          A step counter might fall into this category.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Alias for `add_weight`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Apply the layer on a input.\n",
      " |      \n",
      " |      This simply wraps `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      Assumes that the layer will be built\n",
      " |      to match that input shape provided.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  losses\n",
      " |      Losses which are associated with this `Layer`.\n",
      " |      \n",
      " |      Note that when executing eagerly, getting this property evaluates\n",
      " |      regularizers. When using graph execution, variable regularization ops have\n",
      " |      already been created and are simply returned here.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  name\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.checkpointable.base.CheckpointableBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.contrib.rnn.BasicLSTMCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_vector_size = 100\n",
    "\n",
    "rnn_cell = tf.contrib.rnn.LSTMCell(hidden_vector_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get batch Size from input data rather than hard coding it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_zero_h = tf.matmul(tf.reduce_mean(tf.zeros_like(input_data),2), # 依照第三維取平均, shape=(batch,96)\n",
    "                           tf.zeros([sequence_length,hidden_vector_size])) # shape=(96,100)\n",
    "# final shape=(batch, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial State with tuple of h,c\n",
    "https://zhuanlan.zhihu.com/p/28919765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = tf.contrib.rnn.LSTMStateTuple(initial_zero_h,initial_zero_h)\n",
    "# initial_state是hidden_state以及cell組成的tuple，真正用於lstm循環"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 也可以這樣分開寫，只是後面又需要使用tuple將兩個連在一起\n",
    "# c, h = tf.contrib.rnn.LSTMStateTuple(initial_zero_h,initial_zero_h) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Getting the outputs](https://blog.csdn.net/shuishou07/article/details/78551602)\n",
    "https://stackoverflow.com/questions/44162432/analysis-of-the-output-from-tf-nn-dynamic-rnn-tensorflow-function\n",
    "\n",
    "`tf.nn.dynamic_rnn`：動態計算rnn，因為不一定是每個example的`timesteps`都是一樣長，可以由參數`sequence_length`指定每個examples的有效`timestep`長度，但是前處理還是必須把每個example的`timesteps`padding成一樣長度，指定長度後對於padding的部分就不計算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function dynamic_rnn in module tensorflow.python.ops.rnn:\n",
      "\n",
      "dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None, dtype=None, parallel_iterations=None, swap_memory=False, time_major=False, scope=None)\n",
      "    Creates a recurrent neural network specified by RNNCell `cell`.\n",
      "    \n",
      "    Performs fully dynamic unrolling of `inputs`.\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    ```python\n",
      "    # create a BasicRNNCell\n",
      "    rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)\n",
      "    \n",
      "    # 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]\n",
      "    \n",
      "    # defining initial state\n",
      "    initial_state = rnn_cell.zero_state(batch_size, dtype=tf.float32)\n",
      "    \n",
      "    # 'state' is a tensor of shape [batch_size, cell_state_size]\n",
      "    outputs, state = tf.nn.dynamic_rnn(rnn_cell, input_data,\n",
      "                                       initial_state=initial_state,\n",
      "                                       dtype=tf.float32)\n",
      "    ```\n",
      "    \n",
      "    ```python\n",
      "    # create 2 LSTMCells\n",
      "    rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in [128, 256]]\n",
      "    \n",
      "    # create a RNN cell composed sequentially of a number of RNNCells\n",
      "    multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n",
      "    \n",
      "    # 'outputs' is a tensor of shape [batch_size, max_time, 256]\n",
      "    # 'state' is a N-tuple where N is the number of LSTMCells containing a\n",
      "    # tf.contrib.rnn.LSTMStateTuple for each cell\n",
      "    outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,\n",
      "                                       inputs=data,\n",
      "                                       dtype=tf.float32)\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    Args:\n",
      "      cell: An instance of RNNCell.\n",
      "      inputs: The RNN inputs.\n",
      "        If `time_major == False` (default), this must be a `Tensor` of shape:\n",
      "          `[batch_size, max_time, ...]`, or a nested tuple of such\n",
      "          elements.\n",
      "        If `time_major == True`, this must be a `Tensor` of shape:\n",
      "          `[max_time, batch_size, ...]`, or a nested tuple of such\n",
      "          elements.\n",
      "        This may also be a (possibly nested) tuple of Tensors satisfying\n",
      "        this property.  The first two dimensions must match across all the inputs,\n",
      "        but otherwise the ranks and other shape components may differ.\n",
      "        In this case, input to `cell` at each time-step will replicate the\n",
      "        structure of these tuples, except for the time dimension (from which the\n",
      "        time is taken).\n",
      "        The input to `cell` at each time step will be a `Tensor` or (possibly\n",
      "        nested) tuple of Tensors each with dimensions `[batch_size, ...]`.\n",
      "      sequence_length: (optional) An int32/int64 vector sized `[batch_size]`.\n",
      "        Used to copy-through state and zero-out outputs when past a batch\n",
      "        element's sequence length.  So it's more for performance than correctness.\n",
      "      initial_state: (optional) An initial state for the RNN.\n",
      "        If `cell.state_size` is an integer, this must be\n",
      "        a `Tensor` of appropriate type and shape `[batch_size, cell.state_size]`.\n",
      "        If `cell.state_size` is a tuple, this should be a tuple of\n",
      "        tensors having shapes `[batch_size, s] for s in cell.state_size`.\n",
      "      dtype: (optional) The data type for the initial state and expected output.\n",
      "        Required if initial_state is not provided or RNN state has a heterogeneous\n",
      "        dtype.\n",
      "      parallel_iterations: (Default: 32).  The number of iterations to run in\n",
      "        parallel.  Those operations which do not have any temporal dependency\n",
      "        and can be run in parallel, will be.  This parameter trades off\n",
      "        time for space.  Values >> 1 use more memory but take less time,\n",
      "        while smaller values use less memory but computations take longer.\n",
      "      swap_memory: Transparently swap the tensors produced in forward inference\n",
      "        but needed for back prop from GPU to CPU.  This allows training RNNs\n",
      "        which would typically not fit on a single GPU, with very minimal (or no)\n",
      "        performance penalty.\n",
      "      time_major: The shape format of the `inputs` and `outputs` Tensors.\n",
      "        If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
      "        If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
      "        Using `time_major = True` is a bit more efficient because it avoids\n",
      "        transposes at the beginning and end of the RNN calculation.  However,\n",
      "        most TensorFlow data is batch-major, so by default this function\n",
      "        accepts input and emits output in batch-major form.\n",
      "      scope: VariableScope for the created subgraph; defaults to \"rnn\".\n",
      "    \n",
      "    Returns:\n",
      "      A pair (outputs, state) where:\n",
      "    \n",
      "      outputs: The RNN output `Tensor`.\n",
      "    \n",
      "        If time_major == False (default), this will be a `Tensor` shaped:\n",
      "          `[batch_size, max_time, cell.output_size]`.\n",
      "    \n",
      "        If time_major == True, this will be a `Tensor` shaped:\n",
      "          `[max_time, batch_size, cell.output_size]`.\n",
      "    \n",
      "        Note, if `cell.output_size` is a (possibly nested) tuple of integers\n",
      "        or `TensorShape` objects, then `outputs` will be a tuple having the\n",
      "        same structure as `cell.output_size`, containing Tensors having shapes\n",
      "        corresponding to the shape data in `cell.output_size`.\n",
      "    \n",
      "      state: The final state.  If `cell.state_size` is an int, this\n",
      "        will be shaped `[batch_size, cell.state_size]`.  If it is a\n",
      "        `TensorShape`, this will be shaped `[batch_size] + cell.state_size`.\n",
      "        If it is a (possibly nested) tuple of ints or `TensorShape`, this will\n",
      "        be a tuple having the corresponding shapes. If cells are `LSTMCells`\n",
      "        `state` will be a tuple containing a `LSTMStateTuple` for each cell.\n",
      "    \n",
      "    Raises:\n",
      "      TypeError: If `cell` is not an instance of RNNCell.\n",
      "      ValueError: If inputs is None or an empty list.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.dynamic_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs, state = tf.nn.dynamic_rnn(rnn_cell, input_data, \n",
    "                                   initial_state=initial_state,\n",
    "                                   dtype=tf.float32) # sequence_length預設為None，也就是不需要使用tf.nn.dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.nn.dynamic_rnn`的輸出分成兩部分：\n",
    "* 第一部分`outputs`的shape為`(batch,timesteps,hidden_vector_size)`，也就是`input`的shape`(batch,timesteps,features)`經過計算後得到`(batch, timesteps, hidden_vector_size)`，輸入時每一步`timestep`都會由`features`計算為`hidden_vector_size`，如果只在意最後一步`timesteps`的輸出，只要指定即可，即`outputs[:,-1,:]`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/transpose_1:0' shape=(?, 96, 100) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs #shape(batch,96,100), 每步timestep的輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 100) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 100) dtype=float32>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state # 由最後一步timestep中的(cell,hidden_state)組成的tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use any batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 96, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.06776507, -0.16604237,  0.29889047, ..., -0.2910829 ,\n",
       "        -0.02830626,  0.17666036],\n",
       "       [ 0.0161004 , -0.11812118,  0.28954205, ..., -0.2820639 ,\n",
       "        -0.10455608,  0.19264744],\n",
       "       [ 0.06198378, -0.1680148 ,  0.2578251 , ..., -0.2776524 ,\n",
       "         0.02449181,  0.2434168 ],\n",
       "       ...,\n",
       "       [ 0.01514441, -0.06312319,  0.22368349, ..., -0.22516553,\n",
       "        -0.06407987,  0.21258672],\n",
       "       [ 0.03427761, -0.07078353,  0.30546343, ..., -0.26227936,\n",
       "        -0.10738929,  0.28163087],\n",
       "       [ 0.05872219, -0.13179702,  0.23218733, ..., -0.32039693,\n",
       "        -0.14472461,  0.201169  ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 14\n",
    "fake_input = np.random.uniform(size=[batch_size,96,64])\n",
    "outputs1 = outputs.eval(session=sess,feed_dict={input_data:fake_input}) # sess.run另外一種執行方式\n",
    "print(outputs1.shape) #(batch, timesteps, hidden_vector_size)\n",
    "outputs1[:,-1,:] # 最後一步timestep輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state1 = sess.run(state, feed_dict={input_data:fake_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.14182168, -0.38372934,  0.65008837, ..., -0.60773027,\n",
       "        -0.04967897,  0.4872932 ],\n",
       "       [ 0.03204466, -0.2811767 ,  0.6193227 , ..., -0.5679249 ,\n",
       "        -0.18388133,  0.524578  ],\n",
       "       [ 0.138229  , -0.37704027,  0.523069  , ..., -0.62181395,\n",
       "         0.04411354,  0.6877756 ],\n",
       "       ...,\n",
       "       [ 0.03829008, -0.16699168,  0.4415437 , ..., -0.53708845,\n",
       "        -0.11180121,  0.5098827 ],\n",
       "       [ 0.07112903, -0.16521525,  0.704515  , ..., -0.5881021 ,\n",
       "        -0.19752544,  0.9376542 ],\n",
       "       [ 0.12712282, -0.3374092 ,  0.4719813 , ..., -0.7883067 ,\n",
       "        -0.233245  ,  0.57803166]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(state1.c.shape) # 最後一步timestep的cell值\n",
    "state1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.06776507, -0.16604237,  0.29889047, ..., -0.2910829 ,\n",
       "        -0.02830626,  0.17666036],\n",
       "       [ 0.0161004 , -0.11812118,  0.28954205, ..., -0.2820639 ,\n",
       "        -0.10455608,  0.19264744],\n",
       "       [ 0.06198378, -0.1680148 ,  0.2578251 , ..., -0.2776524 ,\n",
       "         0.02449181,  0.2434168 ],\n",
       "       ...,\n",
       "       [ 0.01514441, -0.06312319,  0.22368349, ..., -0.22516553,\n",
       "        -0.06407987,  0.21258672],\n",
       "       [ 0.03427761, -0.07078353,  0.30546343, ..., -0.26227936,\n",
       "        -0.10738929,  0.28163087],\n",
       "       [ 0.05872219, -0.13179702,  0.23218733, ..., -0.32039693,\n",
       "        -0.14472461,  0.201169  ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(state1.h.shape)  # 最後一步timestep的hidden_stat值, 最後都是由這個hidden_state搭配dense去作為真正輸出\n",
    "state1.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
